
\documentclass[10pt]{report}
\usepackage{amsmath, amsthm, amssymb,mathabx}
\usepackage{cmbright}
\usepackage{euler}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage[margin=2.5cm]{geometry}
\usepackage[applemac]{inputenc}
\usepackage[english]{babel}
\usepackage{verbatim}
\usepackage{algpseudocode}

\setlength{\parindent}{0pt}
\onehalfspace

\newcommand \vv{\vvvert}

\begin{document}

\begin{minipage}[t]{0.58\textwidth}
Technische Universit\"at Berlin\\
Institut f\"ur Mathematik\\
Prof. Dr. J\"org Liesen\\
Carlos Echeverr\'ia\\
Luis Garcia Ramos
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\begin{flushright}
Winter Semester 2014/2015\\
To be submitted in office MA371 on 02.12.2014 before 15.00
\end{flushright}
\end{minipage}
\begin{center}
\textbf{{Numerical Linear Algebra I}}\\
\textbf{Homework 2}
\end{center}

\thispagestyle{empty}

\begin{enumerate}


\item[\textbf{1.}]
We say that two norms $\| \cdot \|, \vvvert \cdot \vvvert$ on a vector space $X$ are \textit{equivalent} if there exist constants $C_1, C_2 > 0$ such that for all $x \in X$
\[C_1 \vvvert x \vvvert \leq \|x\| \leq  C_2\vvvert x\vvvert.\]
It can be shown that \textit{in a finite dimensional vector space, any two norms are equivalent}, and this does not hold for infinite dimensional spaces. For $x =[x_1, \ldots, x_n]^T \in \mathbb{C}^{n}$, recall the following norms: 
\[
\|x\|_2 =\sqrt{\sum_{i=1}^n |x_i|^2},\quad
\|x\|_1 = \sum_{i=1}^n |x_i|,\quad
\|x\|_{\infty} = \max_{i=1, \ldots,n}|x_i|.
\]
\begin{enumerate}
  \item[(a)] Prove the inequalities $\|x\|_2 \leq \|x\|_1 \leq \sqrt{n}\|x\|_2$.
  \item[(b)]Prove the inequalities $ \|x\|_\infty \leq \|x\|_2 \leq  \sqrt{n}\|x\|_{\infty}$
\item[(c)] Conclude that these  three norms are equivalent on $\mathbb{C}^n$.
\end{enumerate}
\vspace{0.4cm} 


\item[\textbf{2.}] The $\infty$-norm on the space of matrices $\mathbb{R}^{n \times n}$ is defined as
\[\|A\|_{\infty}= \sup_{\|x\|_{\infty}=1} \|Ax\|_{\infty}.\]
\begin{enumerate} 
\item[(a)] Show that $\|A\|_{\infty}= \displaystyle{\max_{i=1, \ldots, n}}\sum_{j=1}^n |a_{ij}|$, i.e., the $\infty$-norm of $A$ is the maximum absolute row sum of $A$.
\end{enumerate}
Recall that a norm $\|\cdot\|$ on $\mathbb{R}^{n \times n}$ is \textit{consistent} if $\|AB\| \leq \|A\| \|B\|$ for all $A, B \in \mathbb{R}^{n \times n}$. 
\begin{enumerate} 
\item[(b)]Show that the norm $\|\cdot\|_\infty$ on $\mathbb{R}^{2 \times 2}$ is not consistent.
\item[(c)] Show that the norm $\| \cdot\|$ on $\mathbb{R}^{n \times n}$ with $\|A\|:=n \|A\|_\infty$ is consistent.
\end{enumerate}
\vspace{0.4cm} 
\item[\textbf{3.}] If $(\cdot,\cdot)$ is an inner product on a vector  space $X$, the induced norm $\|x\|= \sqrt{(x, x)}$ satisfies the \textit{parallelogram identity}
\[2 \|x\|^2 + 2\|y\|^2 = \|x+y\|^2 + \|x-y\|^2, \, \text{ for all } x,y \in X.\]
A theorem of P. Jordan and J. von Neumann (1935) states that \textit{a norm in a vector space is induced by an inner product if and only if it satisfies the parallelogram identity}. Using this theorem, show that
\begin{enumerate}
  \item[(a)] The norm $\|\cdot\|_{\infty}$ on $\mathbb{R}^n$ is not induced by an an inner product.
  \item[(b)] The norm $\|\cdot\|_{2}$ on $\mathbb{R}^{2\times 2}$ defined by $\|A\|_2:= \max_{\|x\|=1}\|Ax\|_2$ is not induced by an inner product.
\end{enumerate}









\item[\textbf{4.}] The Householder triangularization method is a backward stable algorithm
  for computing the $QR$ factorization of a matrix. We can illustrate this by a \verb+MATLAB+
  experiment carried out in IEEE double precision arithmetic
  ($\epsilon_{\text{machine}}=2^{-53}\approx 1.11\times10^{-16}$)

  \begin{enumerate}
    \item[(a)] Set $R$ to a $50\times50$ upper-triangular matrix with normal
    random entries.
    \item[(b)] Set $Q$ to a random orthogonal matrix by orthogonalizing a random
    matrix.
    \item[(c)] Set $A$ to the product $QR$, up to rounding errors.
    \item[(d)] Compute the $QR$ factorization $A\approx Q_2R_2$ by Householder
    triangularization 
  \end{enumerate}

  The purpose of these four lines of code is to construct a matrix with a known
  $QR$ factorization, $A=QR$, which can then be compared with the $QR$
  factorization $A=Q_2R_2$ computed by Householder triangularization. Actually,
  because of rounding errors, the $QR$ factors of the computed matrix $A$ are
  not exactly $Q$ and $R$.

\pagebreak

  \item[\textbf{5.}] The task in this exercise is to implement the Gram-Schmidt
  orthogonalization method in two ways which are mathematically equivalent but
  produce different results numerically. The method takes a set of linearly
  independent vectors $v_1,\cdots,v_k\in\mathbb{R}^n$ and computes
  orthonormal vectors $u_1,\cdots,u_k\in\mathbb{R}^n$ such that 
  \text{span}{$v_1,\cdots,v_k$\}=\text{span}\{$u_1,\cdots,u_k$\}. 
  \begin{enumerate}

    \item[(a)] Write a \verb+MATLAB+ function \verb+U = hw2gs(V)+ which
    implements the \textit{classical Gram-Schmidt} algoritm:
    \begin{algorithmic}
    \State \textbf{Input:} $V=[v_1,\ldots,v_k]\in\mathbb{R}^{n\times k}$ 
    \For{$i=1, \ldots, k$} 
    \State $\tilde{u}_i:=v_i-\sum_{j=1}^{i-1}\langle v_i,v_j\rangle u_j$ 
    \State $u_i:=\frac{\tilde{u}_i}{\|\tilde{u}_i\|_2}$
    \EndFor
    \State \textbf{Output:} $U=[u_1,\ldots,u_k]\in\mathbb{R}^{n\times k}$ 
    \end{algorithmic}

    \item[(b)] Write a \verb+MATLAB+ function \verb+U = hw2mgs(V)+ which
    implements the \textit{modified Gram-Schmidt} algoritm:
    \begin{algorithmic}
    \State \textbf{Input:} $V=[v_1,\ldots,v_k]\in\mathbb{R}^{n\times k}$ 
    \For{$i=1, \ldots, k$} 
    \State $\tilde{u}_i:=v_i$
    \For{$j=1,\ldots,i-1$}
    \State $\tilde{u}_i:=\tilde{u}_i-\langle \tilde{u}_i,u_j\rangle u_j$ 
    \EndFor
    \State $u_i:=\frac{\tilde{u}_i}{\|\tilde{u}_i\|_2}$
    \EndFor
    \State \textbf{Output:} $U=[u_1,\ldots,u_k]\in\mathbb{R}^{n\times k}$ 
    \end{algorithmic}
          
    \item[(c)] Examine the level of orthonormality obtained with both
    algorithms.
    This can be done by computing the quantity $F(U):=\|I_k-U^TU\|_{fro}$, which
    measures for the matrix $U$ the departure of the matrix $U^TU$ from the
    identity matrix $I_k$ with respect to the Frobenius-norm. In exact
    arithmetic (i.e. without round-off errors) both algorithms would compute
    a $U$ with $F(U)=0$.

    \item[(d)] Write a \verb+MATLAB+ function \verb+[Fgs,Fmgs]=hw2test()+ wich
    returns two vectors of length $10$. For each $j=1,\ldots,10$ the function
    should generate a random matrix $V_j\in\mathbb{R}^{500\times50j}$. This
    matrix should then be orthonnormalized with both algorithms, i.e.
    $Ugs_j=hw2gs(V_j)$ and $Umgs_j=hw2mgs(V_j)$. The function should return
    $Fgs(j)=F(Ugs_j)$ and $Fmgs(j)=F(Umgs_j)$. The quantities stored in $Fgs$
    and $Fmgs$ should also be visualized in a \textit{single} plot. Useful \verb+MATLAB+ commands are \verb+semilogy+, \verb+rand+, and \verb+hold+.
\end{enumerate}

\end{enumerate}


\end{document}
